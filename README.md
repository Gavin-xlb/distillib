# SPARK
SPARK: Cross-Guided Knowledge Distillation with Spatial Position Augmentation for Medical Image Segmentation

# Structure of this repository
This repository is organized as:

- [datasets](/distillib/datasets/) contains the dataloader for different datasets
- [networks](/distillib/networks/) contains a model zoo for network models
- [scripts](/distillib/scripts/) coontains scripts for preparing data
- [utils](/distillib/utils/) contains api for training and processing data
- [train.py](/distillib/train.py) train a single model
- [train_kd.py](/distillib/train_kd.py) train a student model with KD
- [distiller](/distillib/distiller/) contains a series of distillation methods, either extracted from the code of the original paper or reproduced by myself based on the ideas of the original paper 
- [data_example](/distillib/data_example/) data_example/temp contains a series of images generated by generator_img.py from source dataset
- [gradcam](/distillib/gradcam/) an externally imported module for visualizing the effects of our knowledge distillation method of medical image segmentation
- [output_visual](/distillib/output_visual/) contains several visualizations of the effects of our methods(segmentation results and feature visualization)

# Usage Guide

## Requirements

 All the codes are tested in the following environment:

- pytorch 1.8.0
- pytorch-lightning >= 1.3.7
- OpenCV
- nibabel

The rest of the environment is packaged in the [requirements.txt](/distillib/requirements.txt)

## Dataset Preparation

### KiTS
Download data [here](https://github.com/neheller/kits19)

Please follow the instructions and the ```data/``` directory should then be structured as follows which is needed to be placed under the ```data/kits19/```.
```
data
├── kits19
    ├── data
        ├── case_00000
        |   ├── imaging.nii.gz
        |   └── segmentation.nii.gz
        ├── case_00001
        |   ├── imaging.nii.gz
        |   └── segmentation.nii.gz
        ...
        ├── case_00209
        |   ├── imaging.nii.gz
        |   └── segmentation.nii.gz
        └── kits.json
```
Cut 3D data into slices using ```scripts/SliceMaker.py``` 

```
python scripts/SliceMaker.py --inpath /data/kits19/data --outpath /data/kits --dataset kits --task tumor
```

### LiTS
Download data [here](https://aistudio.baidu.com/datasetdetail/10273)

Similar to KiTS but you may make some adjustments in running ```scripts/SliceMaker.py``` 
```
data
├── lits17
    ├── data
    |    ├── volume-0.nii
    |    ├── volume-1.nii
    |    ...
    |    └── volume-130.nii
    └── segmentation
         ├── segmentation-0.nii
         ├── segmentation-1.nii
         ...
         └── segmentation-130.nii
```
```
python scripts/SliceMaker.py --inpath /data/lits17/data --outpath /data/lits --dataset lits --task tumor
```

## Running
### Training Single Model
Before knowledge distillation, a well-trained teacher model is required. [train.py](/distillib/train.py) is used to training a single model without KD(either a teacher model or a student model). 

[UNet](https://github.com/FENGShuanglang/unet) is recommended to be the teacher model.

```
python train.py --model unet --dateset kits --task tumor
```

After training, the checkpoints will be stored in ```/data/kits/tumor/checkpoints``` as assigned.

If you want to try different models, use ```--model``` with following choices. Similarly, if you want to try LiTS dataset or spliting organ, use ```--dataset``` and ```--task``` with following choices.
```
model: ['deeplabv3+', 'enet', 'erfnet', 'espnet', 'mobilenetv2', 'unet++', 'raunet', 'resnet18', 'unet', 'pspnet', 'mnet', 'resnet34', 'resnet50', 'universeg', 'unet2022']
dataset: ['kits', 'lits']
task: ['organ', 'tumor']
```
### Training With Knowledge Distillation 
For example, use ENet as student model and use UNet as a teacher model

```
python train_kd.py --dataset kits --task tumor --tmodel unet --smodel enet --gpu_id 0 --kd_method SPARK
```

Additionally, you can change student model by revising ```--smodel```, similarly, you can try other methods of knowledge distillation by revising ```--kd_method```.It's worth noting that we don't assigning a value to ```--tckpt``` which is the teacher model weights file, because we define a dict that holds the paths to all the required teacher model weights files.
